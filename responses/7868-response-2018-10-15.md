Okay, so, casual+ consistency!

(I do like how this paper is the first one that we've seen that defines availability
specifically so that it's not satisfied by error responses. I'm going to keep a watch on
that in the future, for systems that assume error responses satisfy availability.)

The main differentiating property of causal consistency is apparently that “dependencies
between operations” (as somehow distinct from causal order) are always preserved. This
cashes out to an extra property on top of causal consistency: all conflicting writes
eventually lead to the same result.

We achieve this by ensuring that write handling is associative and commutative. This means
that all conflicting writes can reach nodes in any order, but as long as two servers have
the same set of writes, they will provide the same data. Combining that with a standard
anti-entropy/gossip protocol to ensure that any given write will eventually reach all
servers, we get the required eventually-property.

The paper argues that causal+ consistency is the strongest known consistency model that's
achievable in an AP system with low latency and linear scalability requirements. It's
provable (how? this sounds like an interesting proof) that sequential consistency cannot
achieve these properties, that causal+ consistency can, and (at least at the time of the
paper, but I believe also today) we know of no consistency model that lies between the
two.

Oh, I see—the “dependencies between operations” is just this paper's way of referring to
causal consistency, which they still consider a huge part of the value proposition of
causal+ consistency. Fair enough, I suppose; as a reader I'd have preferred them to make
it clear that this is a property that causal consistency alone can satisfy, but fine.

They _do_ make it clear that this is the first time causal+ consistency has
been _named_, not _proposed_. They differentiate themselves from previous factors by
providing a better algorithm for actually maintaining causal+ consistency; one that's
meant to have better scaling properties.

Let's dig into that—so previous algorithms maintain logs annotated with version vectors,
and exchange those logs to ensure that conflicting writes are handled in the same
order. This “inhibits” scalability, because…

    …it relies on a single serialization point in each replica to establish ordering. Thus, either
    causal dependencies between keys are limited to the set of keys that can be stored on one node,
    or a single node (or replicated state machine) must provide a commit ordering and log for all
    operations across a cluster.

I don't understand this. What is a “single serialization point in each replica”—is it just
that each node needs to process all incoming writes replicated to it to determine what
order to put it in? If so, why is that an issue? Don't nodes generally have to do a lot of
processing to maintain writes anyway?

This is the first time we've seen a system explicitly assume no partitions within clusters
(this allows them to get away with sharding as a way of providing scalable
linearizability).

The version of their implementation that supports consistent reads across multiple keys
has to maintain old versions of their data to do so. That's interesting—is there some
argument for why that has to be the case?

Their commutative/associative handler is just last-write-wins, only with a consistent
definition of “last” thanks to version vectors. They do have a hook, apparently, for more
intelligent conflict handling—which will lead us into CRDTs pretty nicely I assume.
