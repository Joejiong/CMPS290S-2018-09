# CMPS 290S Reading Response: Naiad: A Timely Dataflow System

## Summary
This paper describes a computational model, timely dataflow, that poses computations as vertices in a dataflow graph. Computation results can stream over edges in the graph, or they can be aggregated over in a vertex and results will flow over edges at the end of batches. This computational model uses _pointstamps_ for lightweight coordination. A pointstamp represents a timestamp that data is received or processed at a given location in the dataflow graph. By constraining pointstamps in various ways, there are certain guarantees that can be achieved about dataflow through the graph, thus enabling a lightweight coordination mechanism. The authors of this paper implemented a distributed prototype of timely dataflow, called Naiad.

## Learning and Understanding
It does not seem entirely clear to me why pointstamps are necessary. The method pair, NotifyAt and OnNotify, provide logical barriers of input batches. The timestamp portion of pointstamps are only allowed to move forward, but they're allowed to arbitrarily move forward (as long as they're unique). Typically in distributed systems timestamps are necessary to disambiguate between operations that happened at different locations that need to be rectified. But for these distributed systems, the effect of a computation could be communicated to any other node. For timely dataflow, a node may contain many vertices of the graph, but the effects of computations have a clear path. My best guess as to the purpose of pointstamps is to handle input from multiple vertices which may progress at varying rates or to handle input from multiple instances of a vertex that do the same computation on the same dataset. But, for both of these scenarios, I still feel as though the combination of the notification logical barrier with the knowledge that data will flow in on specific edges should be unique enough to not need timestamps.

I am curious if there is any work on hinting that certain vertices should be on the same worker, thus allowing some computations to be more granular with less communication cost, or at least reducing communication cost for streaming vertices.

## Research Question and What to Investigate
I think timely dataflow is exactly how I imagine data management systems to process tuples in a hybrid operator- and tuple-at-a-time processing model. With this in mind, I am interested to see how well a datalog-like query language on top of timely dataflow plays with varying backend datastores. This is something that may even be useful for mixing queries/computations from multiple interfaces. That is, perhaps some subset of vertices can be described by a datalog query, while some other subsets of vertices can be described by piped unix commands. I feel like this could be a more interesting approach to something that databricks does, which is connect a user interface (something like a python notebook) to a session that other users can connect to. It may provide a powerful way for scientists to describe computations, but for computer scientists or query optimizers to swap out upstream vertices (producers for vertices defined by scientists) with improved computations/algorithms, etc.

I think it may also be interesting to explore vertices that are optimized for the machine hardware they are running on.
