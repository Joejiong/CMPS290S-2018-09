
    A concurrent computation is linearizable if it is “equivalent,” in a sense formally defined
    in Section 2, to a legal sequential computation.

Let's formally define linearizability!

A history is a sequence of requests and responses, where each request/response is
associated with a particular process and a particular object. A history is “sequential” if
it starts with a request, and every request is followed immediately by a matching
response, and every response is immediately followed by a (matching? why matching?)
request. Two histories are equivalent if ∀ processes, that process sees the exact pattern
of requests and responses under both histories.

Sequential histories can be checked for legality under whatever semantics you want your
objects to have trivially (or at least, that's not the problem this paper is solving).

A history is linearizable if you can

* add 0 or more arbitrary responses to its end
* remove any remaining requests that don't have responses
* and then have a history that is equivalent to some legal sequential history
* such that every case where a request begins strictly after a response in the original
  history, also does so in the sequential history.

That's it? That's it! Let's go home, we're done.

…

There's a proof of locality and a proof of nonblockingness. But the meat here seems to be
in the comparison to serializability: which is linearizability minus the request-response
ordering restriction.

That means that serializable systems are allowed to reorder whole “transactions” to find a
legal history they're equivalent to. Even if process A completes its entire
request/response before process B starts, it's fine for the underlying semantics of the
system to be as if process A operated after process B.

That inherently means that serializable systems have to be blocking and nonlocal, the
paper argues. They do also acknowledge that this means serializable systems can enforce
cross-object invariants, and that it's at least more difficult for linearizable systems to
do the same.

(Really? I mean, I believe them, but I'd like to work through an example. Nothing I see in
the definition of linearizability seems to rule out complex cross-object definitions of
legality, though clearly something must if the property ends up being local.)

I did not at all read them in detail, but the verification section that is the second half
of the paper looks like it's pretty much mechanical. This did catch my eye though—

    So far, linearizability is discussed in terms of histories. This characterization is useful for
    motivating the property, and for demonstrating properties such as locality, but it is awkward
    for verification.

I wonder how Jepsen does it? My surface-level understanding is that it just inherently
checks histories and attempts to reorder them as it can, but maybe this simplification
stated here actually leads to an optimisation it can use? It'd be interesting to check.

Finally, I'm not wholly convinced that the restriction to what they call “well-formed”
histories makes sense—this seems to assume, as far as I can tell, that clients are
well-behaved and the network is reliable. What happens in the presence of duplicate and
dropped messages? Does it cause any problems for the formulation, or just a bunch more
special casing in the proofs?
