So _this_ is the information flow paper!

    the consistency with which we manipulate information should always match or
    exceed the consistency at which we store it.

Hm. Why? If I store some data sequentially consistently, why can I not read it weakly and
use that as input for writing to a weakly consistent cell? Or does “manipulate” here
essentially imply writes to that same cell?

Wait, they have the concept of an atomic transaction on top of this? Across multiple
differingly-consistent objects and data stores? …How? What does that even mean?

They build this on a data store API that requires just the standard read/write/semantics
and a transaction API; okay. (What if the underlying data store doesn't support
transactions? Many designs are single-object stores only…)

A mixed consistency trace is admissible if, for every point in the consistency lattice,
projecting the trace to that consistency level and above is admissible at that consistency
level. That makes a lot of sense, actually.

But we do need to add a notion of noninterference to this: weaker consistent data can't
cause the possible set (under standard nondeterminism) of stronger consistency data to
change. (In Owen's class we learnt that this is actually a relatively weak notion of
noninterference; it's easy to, when using possibilistic noninterference, miss cases where
the _probability_ distribution changes drastically. I'm not sure how much that matters
when the nondeterminism isn't assumed to be under adversary control, but it's a point to
consider.)

This gets us to a standard information-flow analysis of explicit and implicit flow of
consistency levels, ensuring weakly consistent data never impacts strongly consistent
data.

Onto mixed-consistency transactions. Because weakly consistent data can never impact
strongly consistent data (within possibility…), it's reasonable to split a
mixed-consistency transaction into multiple transactions at differing consistency levels,
and order them from strong to weak. (What if they're incomparable? I guess we still have a
noninterference result, so we can concurrently execute the two transactions there.)

As for atomicity…

    To ensure atomicity, _MixT programs_ must be prevented from observing the effects of
    partially committed transactions.

Ah. They're only atomic _within MixT programs_, not within the whole world of agents
observing writes to these data stores. Gotcha. They just write a “don't read from me” note
to the relevant stores and a MixT program is required to suspend reading if it encounters
one of these notes without an associated retraction. Presumably these are written at the
strongest consistency level of the transaction? Again—what if the two strongest are
incomparable?

Oh, this gets hairy with endorsement, yeah. “Pre-endorse” parts of the transaction are
checked to ensure they have no writes, and then it's allowed to run split parts of the
transaction out of order. In some cases you end up needing “don't write to me” notes,
yeesh.

Huh, they have some interesting machinery to “replay” an iterator/loops when a split
transaction has multiple consistency levels within the same loop.

There's a lot of points here where I have Concerns™, where I'm not sure I understand that
the proposed mechanism satisfies the relevant properties. There's a lot of specific,
not-obviously-irreducible complexity that I don't know how to put in context. Apparently
there's a technical report and associated proofs, so sure… but also I'm also not convinced
that possibilistic noninterference is enough to begin with, anyway?
