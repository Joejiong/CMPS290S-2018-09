Response to the paper
Response to ""

Author: Eric Brewer


What is this paper about?

In the article Eric Brewer offers a glimpse of the changes in complexity of a system design since CAP theorem was first announced and proved. He clarified the meaning of C, A, and P in CAP and why partition tolerance is not really optional for a distributed system designer. In the paper, Brewer addressed the abuse of the CAP theorem and gave examples of how application developers choose between Consistency and Availability in real world applications. True to the title, the article was a tour of everything that has happened in the discipline of distributed systems research since Gilbert and Lynch's CAP paper came out.

What's the one thing I learned?

The sections on partition recovery and compensating for mistakes were most insightful. It was good to learn that techniques which may not be necessarily technical (human intervention (even legal!)) are used to recover from mistakes made during network partition. CRDTs and were also something new to me. I intend to look more into CRDTs are their property of being "provably" eventually consistent interests me.

In my opinion the "rules" (if there were any) have not changed. The limit imposed by the CAP theorem still exists but in the article Brewer rightly corrected the misconceptions associated with the CAP theorem. It is important to understand what CAP theorem actually states and what that implies. The idea of picking "2 of 3" is not really accurate as it is not a binary choice.

The idea of merging conflicts during partition recovery was also insightful. It was interesting to look at partition recovery from the perspective of merging conflicts in a version control system or documents in google docs. It was good to get out of Gilbert and Lynch's Read/Write application abstraction to some real world application which added a more practical perspective into the ideas of Consistency and Availability in the face of network partitions.

What I didn't understand?

What's a research-level question I have after having read this paper?

This paper introduced the reader to quite a few ideas in practice in managing consistency and availability in the face of network partition. One important idea that I am interested in was the idea of automating merge conflicts. Brewer mentions merge conflicts and the user of version vectors to resolve conflicts and one result in particular by P. Mahajan et. al which "proved that this kind of causal consistency is the best possible outcome in general if the designer chooses to focus on availability". Although it sounds promising, I wanted to check if there is any basis to complete automation of merge conflicts even on concurrent or near concurrent changes especially from the point of view of commits in a version control system such as git.

What's a concrete step I can take toward answering the research question?

I think any data agnostic proposal would be bound to use either logical clocks or physical clocks to resolve this problem. Concurrent changes happen simultaneously with two or more agents making the changes, having no idea of another change being made and the commit happening at the same (logical or physical time) instant. How can this be resolved? We can look at consensus algorithms which could help resolve this problem. I would say that any consensus algorithm would (probably) not be able to make a proper judgement on concurrent events unless there was a higher intelligence built into the consensus algorithms to process what the data actually meant.
