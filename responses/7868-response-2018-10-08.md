This paper describes “causal memory”, a particular variation(?) of causal consistency
specialised for memory semantics. (Why is this distinct from standard message-passing
systems? The paper suggests that the fact that memory cells are overwritten on new
“messages” allows them to support even more concurrency than the equivalent
message-passing system.)

Formally, the model seems similar to those we've been working with; the only major
difference being that objects only support read and write operations. Sequential
consistency under this model is still what we expect: that there is a valid total order
that respects each individual process' partial order. Pipelined RAM is that for each
process, there is a total order that respects its partial order as well as the partial
order of other processes' writes (but not their reads).

Causal memory adds an additional constraint to the pipelined ram model. Every read is
associated, globally, with exactly one write operation (that actually wrote the value
being read) (except for the initial read). The total order must now respect this
additional induced partial order.

The theoretical? implementation they discuss involve processes sending each other the
writes, along with vector clocks, they're aware of. Other processes accede to those writes
when they find a valid next-step of the partial order; a write to that memory cell that is
exactly one more in vector clock timestamp than the last write they're aware of.

This you can pretty reasonably prove respects the causal memory model, and is nonblocking,
as writes require local checks and may not succeed. However, this nonblocking property
appears to be difficult to achieve in practice, at least judging by the authors'
commentary about their more practical implementations.

The final section of this paper is really interesting. To justify this model as a useful
model, the authors identify certain classes of programs that behave identically under
causal memory as under sequentially consistent memory. That is, they're identifying a
potentially large class of programs that can be coded while _assuming_ the underlying
memory model is sequentially consistent, but that function just as well under causal
memory.

The first class is a toy example: programs that never do concurrent writes. (That is, all
writes are totally ordered by the induced process-specific and causal partial orders.) The
second class is programs that are free of “data races”: defined here as a pair of
operations on one location, not both reads, which are unrelated by the partial order (“are
concurrent”). To be free of data races, it has to be provable that _all_ histories of that
program (that respect the causal memory partial order) contain no data races.

---

The interesting thing about this memory model, to me, is that it seems like it could be
easily implemented in hardware. (It's not clear to me whether that's true or not; the
implementation they give is focused on implementing it via each process, whereas I'd like
to hear whether the actual underlying hardware can gain efficiencies by only exposing this
memory model.)

It'd be really interesting to try and program for such a system, or to develop programming
languages and tools for such a system. In fact—does the data-race-free condition imply
every Rust binary could operate correctly on a causal memory system? Are there physical
implementations of causal memory that we have efforts to create build targets for? Are
they even using the same definition of data races?
