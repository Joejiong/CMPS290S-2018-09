Why does the Amazon shopping cart seem to keep coming up in Shapiro's papers?

This paper outlines a more nuanced view of CRDTs: it's not that you don't need
synchronisation at all, it's that you can move synchronisation _off the critical
path_. CRDTs don't need synchronisation to _function_, they just need it to be
efficient—which is a major advantage as long as you can assume that you will be
partition-free often enough to keep your efficiency.

This “garbage collection” is now given its own formalisation—essentially, once you know
that all operations concurrent with a given operation have been delivered (that operation
is “stable”), you can delete any remaining data about that operation. You can know this
given a liveness assumption—in particular, that replicas don't fail
permanently/undetectably.

Oh, hm, this paper declares its goal to be “quiescent consistency”—much weaker than the
previous paper's strong eventual consistency. I don't see any further discussion of this?

To be a bit more precise about op-based models: they require updates to be delivered _such
that_ downstream operation preconditions are true. This is satisfied by causal delivery,
but doesn't _require_ (I think?) causal delivery.

The paper makes a big deal about causal delivery being a consensus-free system property
that is immune to partitions, but I'm not convinced of this. Don't you end up requiring
something like exactly-once delivery?—a large number of the op-based designs later seem to
depend on this.

## Counters

A G-counter maintains a vector of counters per replica; extending it to support decrements
uses the standard trick of maintaining an incr-vector and a decr-vector. (This puts
Cassandra's counters, as I understand them, in a weird middle place—they maintain a vector
of counters, but they incr and decr them directly, causing their now infamous
overcounting/undercounting anomalies.)

Non-negative counters (i.e., bank account balances) are the first example of “Sadly, the
remaining alternative is to synchronise”—a case where the assumptions of CRDTs don't match
up to the desired semantics. That said, they also point out that these synchronisations
don't have to happen _often_.

## Sets vs MV-registers

MV-registers are the standard obvious CRDT-ish implementation of an arbitrary data type:
conflicting updates are just added to a set of possible updates with associated version
vectors, and a later (possibly-user-visible) update operation will reduce the set to one
element. These do not actually behave like sets; it's more correct to think of the set as
an implementation detail. Apparently Amazon engineers tried to use an MV-register like a
set and noted that removed items would reappear?

That makes sense, as we need additional architecture to ensure set removals are replicated
and commute properly, whether it's tombstone sets (“two-phase”, 2P-sets), or timestamped
adds/removes (LWW-sets), or associated counters (PN-sets), or evidence of observation
(OR-sets).

## Graphs

Wait, why aren't we using OR-sets for the graph vertices/edges? I thought that was the
classic example of where they were useful.

## Text Sequences

Ooh, I like the idea of using a dense identifier space to always be able to generate an
ordered set of identifiers. It's probably not practical—the storage requirements for each
new identifier seems kinda immense—but it feels like there might be something interesting
there.

Tree-based identifier spaces seem much more practical, fine.
