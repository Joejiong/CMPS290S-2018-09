So this is the original paper on the CAP theorem. It outlines a proof-by-contradiction
that no system can be all three of “consistent”, “available”, and “partition tolerant”.

It was really interesting to go back to the root of so much discussion in the field;
certainly there's a lot of aspects of this article that are different from what I'd
expected. For instance, the definitions of the C, A, and P are really distinct. C

Consistency is, as far as I can tell, literally linearizability, which is not
surprising... but availability seems to be interestingly underspecified. I didn't really
understand whether a response indicating failure would qualify for availability—some
proofs seem to assume it would not, but the strict definition is just phrased in terms of
terminating computations.

Partition tolerance is also interesting—it's defined as the ability for the system to
arbitrarily drop any message, which, fair enough. The proofs, however, only use a weaker
statement: that there's a total loss of communication between two parts of the system. It
will be interesting to watch for which definition later papers use—the weaker definition
is strictly implied by the stronger definition, so any paper that relies just on handling
the weaker kind of partitions isn't in general a theoretical counterargument. (Though it
of course could be a practical one.)

The proofs themselves are surprisingly simple. (If your system can't coordinate, and is
forced to not acknowledge that, it can't satisfy coordination-requiring properties!) I'm
not sure I grasp the difference between Theorem 1 and Corollary 1.1 (what, exactly, is a
“fair execution”?)—which means I don't know if I understand why allowing systems to
timeout leaves you able to satisfy it. It appears to be something about not being able to
tell the difference between network failure and latency, though timeouts don't really
solve that problem... they just assign latency past a certain point to be network failure.

The partial consistency result following from that makes sense as a result; I guess the
distinction is that the “asynchronous network model” doesn't give nodes a deterministic
way to pick a partial order to follow?

Finally, the other fascinating thing about this paper is how clearly it states something
that a lot of future commentary will lose—that it's specifically forbidding, for a very
simple reason, one very particular point on the space of possible distributed
systems. That it doesn't say anything about how arbitrarily close you can get, or what the
tradeoff rate is. It does frame the problem later in terms of “pick two of three”;
possibly that leads to thinking that doesn't analyse the gradations here?

That's my next question—I've heard the terms “harvest” and “yield” before, but what do
they actually mean? How do we discuss the gradations? How do we analyse what kinds of
partitions lead to what kinds of availability problems under what kinds of consistency
models? Let's look up these terms and see what has been written on that tradeoff.
