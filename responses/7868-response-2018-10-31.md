Oh, this is an information flow paper! That's _fascinating_.

This is, essentially, an information-flow type system where the metadata being tracked
isn't security levels, but consistency levels. Consistency levels are assigned to — the
paper says to “ADT instances”, but it reads to me to be more on names (the things which
are traditionally typed.)

That way, we can annotate data from the underlying system at varying consistency levels
(strong, or “dynamic”, where the system is allowed to relax consistency for performance
within provided error bounds or latency bounds). Then the type system ensures that code
that requires highly consistent data isn't passed data of weaker provenance than it needs.

The central proof is a noninterference proof, as standard, with endorsement as the
standard escape hatch.

Available consistency types:

* `Rushed[T]`, for latency bounds, as a union of the possible consistency levels available
  within that latency bound (so the application can behave differently on lower/higher
  consistency data received within that bound)
* `Interval[T]`, for accuracy bounds. Given some understanding of the impact of various
  kinds of weak consistency on a datatype, (the paper uses the example of a counter), we
  can provide a range such that the true value is within the range.

(Wait, really? That's not how I generally think of confidence/credible intervals—I'm used
to thinking of them as a range such that there's an (1-x)% _chance_ the true value is
within that range. It looks like they use a reservation system with some tweaks to manage
the maximum number of unseen writes you could have at any point, adjusting that during
runtime... which makes sense, and I can see how that's more useful in this context than
trying to put numbers on sources of nondeterminism. Still, something about this feels
weird, to me—that it's size of the bound and not the probability of the bound that's fixed
beforehand scares me in a way I can't easily identify.)

The latency bound implementation by default issues multiple requests at different
consistency levels, and returns the strongest one that returns within the latency
bound. Presumably you can do better, but sure, that's a reasonable default
implementation. To ensure this doesn't cause undue load, each client keeps track of
observed load by consistency level (similarities to the TCP-backoff algorithm, I think?)
and doesn't make stronger consistency read requests when they look unlikely to return
within the latency bound.

I like this paper, though it doesn't formalise itself quite enough for my liking. I'd like
to see a formal specification of the language, a better description of the lattice (Is an
interval of 5% of T a subtype of an interval of 10% of T?), as well as an actual
noninterference proof. Not that I think any of those will be nontrivial, but just to guard
against the possibility that something about this domain makes them subtle or interesting.
