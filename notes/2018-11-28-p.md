# Rough notes from my reading of the P paper

## Key takeaways from this paper

We're reading this paper even though it doesn't claim to be specifically about distributed programming, because P's "interacting state machines that communicate with each other using events" model is arguably a good fit for distributed programming, especially because P comes with facilities for fault tolerance.  Asynchrony, fault tolerance, and message-passing -- key ingredients of P -- are arguably key ingredients of distributed programming in general.  Also, follow-up work on P (namely, the ModP paper at OOPSLA '18) does more specifically focus on distributed programming.

P programs are designed to be easy to test; the notion of testing is built in.  "Ghost machines" let you specify the environment in which to test a program.  (In ModP, this notion of "ghost machines" is replaced by abstract module interfaces.)

### Section 1

> A P program is a collection of state machines communicating via events. Each state machine has a collection of states, local variables, and actions.  The states and actions are annotated with code statements to read and update local variables, send events to other state machines, raise events locally, or call external C functions.  The external C functions are used to write parts of the code that have do with data transfer.

This is in line with how, in the distsys papers we've read, we generally ignore how the communication layer is implemented.

> Our notion of responsiveness is weaker than synchronous languages like Esterel [4] (which require input events to be handled synchronously during every clock tick, and are hence too strong to be implemented in asynchronous software), but stronger than purely asynchronous languages like Rhapsody [12] (where asynchronous events can be queued arbitrarily long before being handled).

So is it "partial synchrony" (like in the PSync paper) where there's a time bound on how long it'll take to handle an event?  There's a notion of "deferred events", but also a way to ensure events won't be deferred forever.  So, yes, I think this does meet the definition of "partial synchrony" -- there is *some* bound (although we might not be able to say in advance what it is) on how long messages will be delayed.  (I'm conflating messages and events here.)

A notion of systematic testing is baked into P using a model checker.  There are an unbounded number of executions to explore, but "the enumeration is controlled by bounding techniques" like depth-bounding and delay-bounding.

(The stuff about bounding reminds me of the Q9 paper that just came out at OOPSLA -- Safe Replication through Bounded Concurrency Verification -- they called their system Q9 because they claim that "most replication anomalies manifest under 9 or fewer concurrent operations". http://kcsrk.info/papers/oopsla18-q9.pdf)

### Section 2

> A P program is a collection of machines.  Machines communicate with each other asynchronously through events. Events are queued, but machines are required to handle them in a responsive manner (defined precisely later)—failure to handle events is detected by automatic verification.

Yeah, sounds like partial synchrony.

Reading the description of the state machines, the "call transitions" complicate things a bit beyond normal state machines.  They introduce a call-stack-like abstraction.  Seems like this could've been implemented just as a normal state machine, although it might've been messier.

> If an event e arrives in a state n, and there is no transition defined for e, then the verifier flags an “unhandled event” violation. There are certain circumstances under which the programmer may choose to delay handling of specific events or ignore the events by dropping them. These need to be specified explicitly by marking such events in the associated deferred set, so that they are not flagged by the verifier as unhandled. The verifier also implements a liveness check that prevents deferring events indefinitely. This check avoids trivial ways to silence the verifier by making every event deferred in every state.

This is cool.  I like the "deferred set" abstraction.  I feel like it's a simple idea, but without a DSL to express it, it would lead to lots of spaghetti code.  The liveness check is cool, too.

I'm a bit confused about the elevator example.  It doesn't seem to include anything about the elevator moving between floors, or about the floor doors.  Users only open and close the doors; they don't push buttons for floors.

### Section 3

> As mentioned in Section 2, a state declaration consists of a name n, a set of events (called deferred set), and two statements: (1) an entry statement and (2) an exit statement.

This is actually a bit different from Section 2, where there was no exit statement and instead there was a set of action handlers.

> P programs manage memory manually by using the new and delete commands.  The new command allocates a new instance of a machine and returns its reference, and the delete command terminates the machine which executes the command and frees its resources. It is the responsibility of the P programmer to perform cleanup and ensure absence of dangling references, or pending message exchanges before calling delete.  Manually managing the memory add some complexity in order to retain a precise control over the footprint of the program.

In a distributed setting, how do you know when it's okay to free a machine?  What if you're still expecting messages from it?  You (apparently) know you don't have to wait unboundedly for messages, but how long *should* you wait?

> A global configuration M is a map from a machine identifier to a tuple representing the machine configuration.

The semantics in fig. 4 is defined on these global configurations.  A global configuration just maps machine IDs to individual machine configurations.

An individual machine configuration is the tuple (gamma, sigma, s, q) where gamma is the call stack, sigma is the environment variable bindings, s is the statements still to be executed on the machine, and q is the input buffer on the machine.  (LK: I think this should have been an upper-case S.)

(Not 100% sure how the graphical language maps onto this.)

> These rules use -raise- and -return-, which are dynamic instances of raise and return statements respectively

I think by "dynamic instances" this just means these aren't part of the surface language but instead are introduced by the evaluation rules for the surface-language "raise" and "return".

Man, the operational semantics is really hairy, and it isn't even complete!  It leaves out the FFI and a *different* kind of call transition that saves a continuation.  I'm glad they didn't try to also fit these in.  But I wonder if it really had to be so hairy.

> Beyond providing constructs for building safe programs, the design of the P language also contains constructs to build responsive programs. Explicitly deferring messages instead of doing so implicitly is such a design choice. However, it is still possible to excessively defer events, thus not processing them.

"Responsiveness" sounds not unlike what we've been calling "availability".

> Therefore, we propose two liveness properties that must be satisfied by a P program. We describe these two properties below in terms of a precise description of those infinite executions which violate these properties; we use linear temporal logic [18] for our specifications.

Note that we need the descriptions of executions that violate the properties to be infinite because these are liveness properties.  Here are the two descriptions:

In LTL, diamond is "finally" and box is "globally".  (or "eventually" and "always" as I like to read them).

1) Executions where "there exists an m where eventually always m is enabled and takes a step" are erroneous.

fair(m) means that "always eventually (m is enabled implies that m takes a step)".  So:

2) Executions where "for all m,  fair(m) and there exist m, e, m', such that eventually (m enqueues e into m' and m' never dequeues e)" are erroneous.

But then they temper this second one a bit by allowing "low-priority" events to be postponed forever.

The type system is simple -- not even described.  I wonder if a fancier type system for P would make the operational semantics less hairy.

### Section 4

The paper makes it sound like P programs just compile to a bunch of data structures.  The language's operational semantics is all captured by the (C) runtime library.

(Where's the code that actually executes an action, like opening the elevator door?  Maybe this is done through the FFI.)

I wish they would say a little more about how the P-to-C compiler itself was developed.

### Section 5

> if an error occurs in an execution with a more fine-grained context-switching, it can be shown to occur in another equivalent execution in which context-switches happen only at the points mentioned above.

This is cool.  Apparently this kind of thing is known as "atomicity reduction" in the model checking literature.  (I wonder if LDFI does anything like this.)

Is the idea of Figure 7 to show that eventually every graph levels off, indicating that at some not-too-large delay bound, we explore all the states?  (At least, it looks like that's what's going on; but do we *know* that that's the case?)

### Section 6

> the verifier runs took several hours to finish (even after using multicores to scale the state exploration)

Hmm, to what extent was this parallelizable?  It's really hard to parallelize most SMT solving, for instance, because the sub-problems aren't particularly independent.

The later work on ModP, however, lends itself to parallelization because independent modules can be checked in parallel.


